{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import snscrape.modules.twitter as sntwitter\n",
    "\n",
    "import pandas as pd\n",
    "from textblob import TextBlob\n",
    "import re\n",
    "\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colors\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "from PIL import Image\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EXTRACTING TWEETS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attributes_container = []\n",
    "search_query = \"keyword since:2023-06-05_23:59:00_EEST until:2023-06-09 lang:en\"\n",
    "mode_param = sntwitter.TwitterSearchScraperMode.TOP\n",
    "\n",
    "# Using TwitterSearchScraper to scrape data and append tweets to list\n",
    "for i,tweet in enumerate(sntwitter.TwitterSearchScraper(search_query, mode = mode_param).get_items()):\n",
    "    if i>500:\n",
    "        break\n",
    "    attributes_container.append([tweet.rawContent])\n",
    "    \n",
    "\n",
    "df = pd.DataFrame(attributes_container, columns=[\"Tweet\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CLEANING TEXT FOR ANALYSIS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleantext(text):\n",
    "    \n",
    "    text = re.sub(r'https?:\\/\\/\\S+', '', text) #links\n",
    "    text = re.sub(r'RT[\\s]+', '', text) #removed RT\n",
    "    text = re.sub(r'#[A-Za-z0-9_]+', '', text) #removed '#'\n",
    "    text = re.sub(r'@[A-Za-z0-9]+', '', text) #removed mentions\n",
    "    text = re.sub(r'[^0-9A-Za-z \\t]+', '', text) #removed non alphanumeric\n",
    "    text = text.lower() # convert text to lowercase\n",
    "\n",
    "    return text\n",
    "\n",
    "def remove_emojis(text):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                               u\"\\U00002702-\\U000027B0\"  # other miscellaneous symbols\n",
    "                               u\"\\U000024C2-\\U0001F251\" \n",
    "                               \"]+\", flags=re.UNICODE)\n",
    "    # Remove emojis from the text\n",
    "    text_without_emojis = emoji_pattern.sub(r'', text)\n",
    "    return text_without_emojis\n",
    "\n",
    "df['Tweet'] = df['Tweet'].apply(cleantext)\n",
    "df['Tweet'] = df['Tweet'].apply(remove_emojis)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SENTIMENT ANALYSIS WITH roBERTa**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from scipy.special import softmax\n",
    "from transformers import TFAutoModelForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = f\"cardiffnlp/twitter-roberta-base-sentiment\" #PretrainedModel\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSentiment(text):\n",
    "    max_length = 512  # Maximum length supported by the model\n",
    "\n",
    "    # Truncate or pad the text to match the expected length\n",
    "    encoded_input = tokenizer(text, truncation=True, padding='max_length', max_length=max_length, return_tensors=\"pt\")\n",
    "    \n",
    "    outputs = model(**encoded_input)\n",
    "    logits = outputs.logits\n",
    "    sentiment = logits.argmax().item()\n",
    "    if sentiment == 0:\n",
    "        return 'Negative'\n",
    "    elif sentiment == 1:\n",
    "        return 'Neutral'\n",
    "    else:\n",
    "        return 'Positive'\n",
    "\n",
    "tweets = df['Tweet']\n",
    "\n",
    "sentiment_df_roberta = pd.DataFrame()\n",
    "\n",
    "for post in tqdm(tweets):\n",
    "    sentiment = getSentiment(post)\n",
    "    row = pd.Series([sentiment, post], index=['Tweet_Sentiment', 'Tweet'])\n",
    "    sentiment_df_roberta = pd.concat([sentiment_df_roberta, row.to_frame().T])\n",
    "\n",
    "sentiment_df_roberta.reset_index(drop=True, inplace=True)\n",
    "sentiment_df_roberta = sentiment_df_roberta.rename_axis('Tweet_No')\n",
    "print(sentiment_df_roberta.head())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**VISUALIZATION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = sentiment_df_roberta['Tweet_Sentiment'].value_counts() \n",
    "\n",
    "color_codes = [\"#F2543D\", \"#EEEEEE\", \"#38C477\"]\n",
    "sns.set_palette(color_codes)\n",
    "labels = ['Негативна', 'Нейтральна', 'Позитивна']\n",
    "plt.pie(counts, labels=labels, autopct='%.0f%%')\n",
    "sns.set(font='Calibri')\n",
    "\n",
    "plt.title('Тональність твітів',loc='right',color='red')\n",
    "plt.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Word Popularity using N-gram**\n",
    "\n",
    "tokenizing, removing the stop words, and stemming on previously cleaned texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pop_list = df.copy()\n",
    "def remove_punct(text):\n",
    "    text = \"\".join([char for char in text if\n",
    "                    char not in string.punctuation])\n",
    "    text = re.sub('[0-9]+', '', text)\n",
    "    text = text.strip()  #ADDED\n",
    "    return text\n",
    " \n",
    " \n",
    "pop_list['punct'] = pop_list['Tweet'].apply(\n",
    "  lambda x: remove_punct(x))\n",
    " \n",
    "# Applying tokenization\n",
    "def tokenization(text):\n",
    "    text = re.split('\\W+', text)\n",
    "    text = [t.strip() for t in text] #ADDED\n",
    "    return text\n",
    " \n",
    " \n",
    "pop_list['tokenized'] = pop_list['punct'].apply(\n",
    "    lambda x: tokenization(x.lower()))\n",
    " \n",
    "# Removing stopwords\n",
    "#stopword = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "my_stopwords = ['ukrainian', 'go','president','volodymyr']\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "final_sw = my_stopwords + stopwords\n",
    "#stopwords.extend(my_stopwords)\n",
    "def remove_stopwords(text):\n",
    "    text = [word for word in text if\n",
    "            word not in final_sw]\n",
    "    return text\n",
    " \n",
    "pop_list['nonstop'] = pop_list['tokenized'].apply(\n",
    "  lambda x: remove_stopwords(x))\n",
    " \n",
    "# Applying Stemmer\n",
    "ps = nltk.PorterStemmer() \n",
    " \n",
    "def stemming(text):\n",
    "    text = [ps.stem(word) for word in text]\n",
    "    return text\n",
    " \n",
    "pop_list['stemmed'] = pop_list['nonstop'].apply(\n",
    "  lambda x: stemming(x))\n",
    " \n",
    "pop_list.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Most used words**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleantext(text):\n",
    "    text = remove_punct(text)\n",
    "    text = tokenization(text.lower())\n",
    "    text = remove_stopwords(text)\n",
    "    text = stemming(text) \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying Countvectorizer\n",
    "countVectorizer = CountVectorizer(analyzer=cleantext)\n",
    "countVector = countVectorizer.fit_transform(pop_list['Tweet'])\n",
    "count_vect_df = pd.DataFrame(\n",
    "    countVector.toarray(),\n",
    "  columns=countVectorizer.get_feature_names_out())\n",
    "count_vect_df.head()\n",
    " \n",
    "# Most Used Words\n",
    "count = pd.DataFrame(count_vect_df.sum())\n",
    "countdf = count.sort_values(0,\n",
    "                            ascending=False).head(20)\n",
    "countdf = countdf.rename(columns={0: 'Word Count'})\n",
    "countdf[0:16]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bigram and Trigram**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_n_gram(corpus, ngram_range, n=None):\n",
    "    vec = CountVectorizer(ngram_range=ngram_range,\n",
    "                          stop_words=final_sw).fit(corpus)\n",
    "    bag_of_words = vec.transform(corpus)\n",
    "    sum_words = bag_of_words.sum(axis=0)\n",
    "    words_freq = [(word, sum_words[0, idx])\n",
    "                  for word, idx in vec.vocabulary_.items()]\n",
    "    words_freq = sorted(words_freq, key=lambda x: x[1], reverse=True)\n",
    "    return words_freq[:n]\n",
    " \n",
    "# n2_bigram\n",
    "n2_bigrams = get_top_n_gram(pop_list['Tweet'], (2, 2), 20)\n",
    "plt.figure(figsize=(10, 6),\n",
    "           dpi=600)  # Push new figure on stack\n",
    "sns_plot = sns.barplot(x=1, y=0, data=pd.DataFrame(n2_bigrams))\n",
    "#plt.savefig('bigram.jpg')  # Save that figure\n",
    "# n3_trigram\n",
    "n3_trigrams = get_top_n_gram(pop_list['Tweet'], (3, 3), 20)\n",
    " \n",
    "plt.figure(figsize=(8, 6),\n",
    "           dpi=600)  # Push new figure on stack\n",
    "sns_plot = sns.barplot(x=1, y=0, data=pd.DataFrame(n3_trigrams))\n",
    "#plt.savefig('trigram.jpg')  # Save that figure"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
