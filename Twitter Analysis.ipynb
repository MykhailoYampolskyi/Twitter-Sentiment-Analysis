{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9eddfd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import snscrape.modules.twitter as sntwitter\n",
    "import pandas as pd\n",
    "from textblob import TextBlob\n",
    "from wordcloud import WordCloud\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colors\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import nltk\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "from PIL import Image\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d015335",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daaed5ca",
   "metadata": {},
   "source": [
    "**EXTRACTING TWEETS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e355dd06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating list to append tweet data to\n",
    "\n",
    "attributes_container = []\n",
    "search_query = \"Zelensky since:2023-03-25 until:2023-03-27 lang:en\"\n",
    "mode_param = sntwitter.TwitterSearchScraperMode.TOP\n",
    "\n",
    "# Using TwitterSearchScraper to scrape data and append tweets to list\n",
    "for i,tweet in enumerate(sntwitter.TwitterSearchScraper(search_query, mode = mode_param).get_items()):\n",
    "    if i>1000:\n",
    "        break\n",
    "    attributes_container.append([tweet.rawContent])\n",
    "    \n",
    "\n",
    "df = pd.DataFrame(attributes_container, columns=[\"Tweet\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e71befa1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "display(df.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee7f2ae6",
   "metadata": {},
   "source": [
    "**CLEANING TEXT FOR ANALYSIS**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d0feea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleantext(text):\n",
    "    \n",
    "    text = re.sub(r'https?:\\/\\/\\S+', '', text) #links\n",
    "    text = re.sub(r'RT[\\s]+', '', text) #removed RT\n",
    "    text = re.sub(r'#[A-Za-z0-9_]+', '', text) #removed '#'\n",
    "    text = re.sub(r'@[A-Za-z0-9]+', '', text) #removed mentions\n",
    "    text = re.sub(r'[^0-9A-Za-z \\t]+', '', text) #removed non alphanumeric\n",
    "    text = text.lower() # convert text to lowercase\n",
    "\n",
    "    return text\n",
    "\n",
    "def remove_emojis(text):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                               u\"\\U00002702-\\U000027B0\"  # other miscellaneous symbols\n",
    "                               u\"\\U000024C2-\\U0001F251\" \n",
    "                               \"]+\", flags=re.UNICODE)\n",
    "    # Remove emojis from the text\n",
    "    text_without_emojis = emoji_pattern.sub(r'', text)\n",
    "    return text_without_emojis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1907644",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df['Tweet'] = df['Tweet'].apply(cleantext)\n",
    "df['Tweet'] = df['Tweet'].apply(remove_emojis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c65c70c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#display(df.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25457704",
   "metadata": {},
   "source": [
    "**SENTIMENT ANALYSIS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d55a8f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#using VADER\n",
    "def getSentiment(text):\n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "    vs = analyzer.polarity_scores(text)\n",
    "    compound_score = vs['compound']\n",
    "    if compound_score < 0:\n",
    "        return 'Negative'\n",
    "    elif compound_score == 0:\n",
    "        return 'Neutral'\n",
    "    else:\n",
    "        return 'Positive'\n",
    "\n",
    "tweets = df['Tweet']\n",
    "\n",
    "sentiment_df = pd.DataFrame()\n",
    "\n",
    "for post in tqdm(tweets):\n",
    "    sentiment = getSentiment(post)\n",
    "    row = pd.Series([sentiment, post], index=['Tweet_Sentiment', 'Tweet'])\n",
    "    sentiment_df = pd.concat([sentiment_df, row.to_frame().T])\n",
    "\n",
    "sentiment_df.reset_index(drop=True, inplace=True)\n",
    "sentiment_df = sentiment_df.rename_axis('Tweet_No')\n",
    "print(sentiment_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed33314",
   "metadata": {},
   "outputs": [],
   "source": [
    "#USING TEXTBLOB\n",
    "# def getPolarityScore(text):\n",
    "#     return TextBlob(text).sentiment.polarity \n",
    "\n",
    "# def getSentiment(polarity_score):\n",
    "#     if polarity_score < 0:\n",
    "#          return 'Negative'\n",
    "#     elif polarity_score == 0:\n",
    "#          return 'Neutral'\n",
    "#     else:\n",
    "#          return 'Positive'\n",
    "\n",
    "# tweets = df['Tweet']\n",
    "\n",
    "# sentiment_df = pd.DataFrame()\n",
    "\n",
    "# for post in tqdm(tweets):\n",
    "#     polarity = getPolarityScore(post)\n",
    "#     sentiment = getSentiment(polarity)\n",
    "#     row = pd.Series([round(polarity, 2), sentiment, post], index=['Tweet_Polarity', 'Tweet_Sentiment', 'Tweet'])\n",
    "#     sentiment_df = pd.concat([sentiment_df, row.to_frame().T])\n",
    "\n",
    "# sentiment_df.reset_index(drop=True, inplace=True)\n",
    "# sentiment_df = sentiment_df.rename_axis('Tweet_No')\n",
    "# print(sentiment_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6982c3c2",
   "metadata": {},
   "source": [
    "**VISUALIZATION**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a917f4d",
   "metadata": {},
   "source": [
    "Bar graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f9b2523",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# plt.figure(figsize=(8, 6))\n",
    "\n",
    "# sns.countplot(x=\"Tweet_Sentiment\", data=sentiment_df)\n",
    "# plt.xlabel(\"Count per Sentiment\")\n",
    "# plt.title(\"Count of sentiment in Dataset\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a927ef8a",
   "metadata": {},
   "source": [
    "Pie Chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a33f42f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = sentiment_df['Tweet_Sentiment'].value_counts() \n",
    "\n",
    "colors = ['red', 'green', 'grey']\n",
    "counts.plot.pie(autopct='%.0f%%', colors=colors)\n",
    "plt.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd7e0edb",
   "metadata": {},
   "source": [
    "**Word Popularity using N-gram**\n",
    "\n",
    "tokenizing, removing the stop words, and stemming on previously cleaned texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "804c10d0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pop_list = df.copy()\n",
    "def remove_punct(text):\n",
    "    text = \"\".join([char for char in text if\n",
    "                    char not in string.punctuation])\n",
    "    text = re.sub('[0-9]+', '', text)\n",
    "    text = text.strip()  #ADDED\n",
    "    return text\n",
    " \n",
    " \n",
    "pop_list['punct'] = pop_list['Tweet'].apply(\n",
    "  lambda x: remove_punct(x))\n",
    " \n",
    "# Applying tokenization\n",
    "def tokenization(text):\n",
    "    text = re.split('\\W+', text)\n",
    "    text = [t.strip() for t in text] #ADDED\n",
    "    return text\n",
    " \n",
    " \n",
    "pop_list['tokenized'] = pop_list['punct'].apply(\n",
    "    lambda x: tokenization(x.lower()))\n",
    " \n",
    "# Removing stopwords\n",
    "#stopword = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "my_stopwords = ['ukrainian', 'go','president','volodymyr']\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "final_sw = my_stopwords + stopwords\n",
    "#stopwords.extend(my_stopwords)\n",
    "def remove_stopwords(text):\n",
    "    text = [word for word in text if\n",
    "            word not in final_sw]\n",
    "    return text\n",
    " \n",
    "pop_list['nonstop'] = pop_list['tokenized'].apply(\n",
    "  lambda x: remove_stopwords(x))\n",
    " \n",
    "# Applying Stemmer\n",
    "ps = nltk.PorterStemmer() \n",
    " \n",
    "def stemming(text):\n",
    "    text = [ps.stem(word) for word in text]\n",
    "    return text\n",
    " \n",
    "pop_list['stemmed'] = pop_list['nonstop'].apply(\n",
    "  lambda x: stemming(x))\n",
    " \n",
    "pop_list.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d376ca05",
   "metadata": {},
   "source": [
    "**Most used words**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb6301a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleantext(text):\n",
    "    text = remove_punct(text)\n",
    "    text = tokenization(text.lower())\n",
    "    text = remove_stopwords(text)\n",
    "    text = stemming(text) \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c19cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying Countvectorizer\n",
    "countVectorizer = CountVectorizer(analyzer=cleantext)\n",
    "countVector = countVectorizer.fit_transform(pop_list['Tweet'])\n",
    "count_vect_df = pd.DataFrame(\n",
    "    countVector.toarray(),\n",
    "  columns=countVectorizer.get_feature_names_out())\n",
    "count_vect_df.head()\n",
    " \n",
    "# Most Used Words\n",
    "count = pd.DataFrame(count_vect_df.sum())\n",
    "countdf = count.sort_values(0,\n",
    "                            ascending=False).head(20)\n",
    "countdf = countdf.rename(columns={0: 'Word Count'})\n",
    "countdf[0:16]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a2c0b5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.bar(countdf.index, countdf['Word Count'])\n",
    "\n",
    "plt.title(\"Most Used Words\")\n",
    "plt.xlabel(\"Words\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.xticks(rotation=45, ha='right', fontsize=8)\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42fcda2c",
   "metadata": {},
   "source": [
    "**Bigram and Trigram**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bb5d029",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_n_gram(corpus, ngram_range, n=None):\n",
    "    vec = CountVectorizer(ngram_range=ngram_range,\n",
    "                          stop_words=final_sw).fit(corpus)\n",
    "    bag_of_words = vec.transform(corpus)\n",
    "    sum_words = bag_of_words.sum(axis=0)\n",
    "    words_freq = [(word, sum_words[0, idx])\n",
    "                  for word, idx in vec.vocabulary_.items()]\n",
    "    words_freq = sorted(words_freq, key=lambda x: x[1], reverse=True)\n",
    "    return words_freq[:n]\n",
    " \n",
    "# n2_bigram\n",
    "n2_bigrams = get_top_n_gram(pop_list['Tweet'], (2, 2), 20)\n",
    "plt.figure(figsize=(10, 6),\n",
    "           dpi=600)  # Push new figure on stack\n",
    "sns_plot = sns.barplot(x=1, y=0, data=pd.DataFrame(n2_bigrams))\n",
    "plt.savefig('bigram.jpg')  # Save that figure\n",
    "# n3_trigram\n",
    "n3_trigrams = get_top_n_gram(pop_list['Tweet'], (3, 3), 20)\n",
    " \n",
    "plt.figure(figsize=(8, 6),\n",
    "           dpi=600)  # Push new figure on stack\n",
    "sns_plot = sns.barplot(x=1, y=0, data=pd.DataFrame(n3_trigrams))\n",
    "plt.savefig('trigram.jpg')  # Save that figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f06a12c",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "twenv",
   "language": "python",
   "name": "twenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
